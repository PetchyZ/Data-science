# -*- coding: utf-8 -*-
"""UFO Project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16F4QYiJjieXubCqjFBysm1UeaFeoG9ih
"""

!gdown --id 1koPiXeZujrFrLnJOIctJAka45Pwr1cpE
!unzip Data_UFO.zip -d "/content/drive/My Drive/Colab Notebooks/Data/"

#Mount Drive
from google.colab import drive
drive.mount('/content/drive')

"""# Import Library

import library ต่างๆ
"""

#import from drive
import pandas as pd 
import numpy as np
import datetime
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

import folium
from branca.element import Figure
from sklearn.model_selection import train_test_split 
local = "/content/drive/MyDrive/data_set/"
data = pd.read_csv(local+"UFO_Sighting_Main.csv")
data.info()

"""# Cleaning

Cleaning Data1
"""

#EDA
#cleanning data
#data01
#latitude
#Error 1 item because q is contains 
data['latitude'][data['latitude'].str.contains('q') == True] = np.nan
print(data.iloc[43782])
data['latitude'] = data['latitude'].astype(float)
data.info()

#duration(seconds)
#Error with '
print("===========Before format===========")
print(data['duration (seconds)'][data['duration (seconds)'].str.contains('`') == True])
for i in data['duration (seconds)'][data['duration (seconds)'].str.contains('`') == True]:
  data['duration (seconds)'][data['duration (seconds)'].str.contains('`') == True] = data['duration (seconds)'][data['duration (seconds)'].str.contains('`') == True][:-1]
print(data['duration (seconds)'][data['duration (seconds)'].str.contains('`') == True])
data['duration (seconds)'] = data['duration (seconds)'].astype(float)

#date time เป็น string แปลงเป็น time stamp
# find datetimes which have '24:00' and rewrite
#ใน timestamp เก็บค่า 0-23 เพียงเท่านั้นเพราะฉนั้นต้องเปลี่ยน 24:00 ไปเป็น 00:00
tw_hours = data['datetime'].str[-5:] == '24:00'
data.loc[tw_hours, 'datetime'] = data['datetime'].str[:-5] + '00:00'

# construct datetime series 
#ทำการจับค่าเปลี่ยนเป็น timestamp ตาม format 
data['new_datetime'] = pd.to_datetime(data['datetime'], format='%m/%d/%Y %H:%M')

# add one day where applicable
#ตามหา item ที่มีค่า date = 24:00 แล้วแปลงเป็นขึ้นวันใหม่
data.loc[tw_hours, 'new_datetime'] += pd.DateOffset(1)


#get format='%d/%m/%Y %H:%M')
data['datetime'] = pd.to_datetime(data['new_datetime'], format='%d%b%Y %H:%M')
data.head()
data.pop('new_datetime')

#
print("===========After format==========")
data.info()

"""Cleaning Data2"""

#cleaning data2 have hoax need to process
data2 = pd.read_csv(local+'#ThrowbackDataThursday_UFOs.csv')
data2 = data2.loc[data2['Hoax']=='No']
data2.pop('Hoax')
#data set to 12 PM and 12 AM timeset need to convert to 24 hours
#data_test = pd.to_datetime('8/13/1943 10:00:00 PM', format='%m/%d/%Y %I:%M:%S %p')
data2['n_datetime'] = pd.to_datetime(pd.to_datetime(data2['datetime'], format='%m/%d/%Y %I:%M:%S %p'), format='%d%b%Y %H:%M')
data2['datetime'] = data2['n_datetime']
data2.pop('n_datetime')
data2.head()
Big_data = pd.concat([data['datetime'],data['city'],data['state'],data['country'],data['shape'],data['duration (seconds)'],data['duration (hours/min)'],data['comments'],data['date posted'],data['latitude'],data['longitude ']],axis=1)
Big_data = pd.concat([Big_data,data2])
Big_data.iloc[80332] #check connection 
Big_data.info()

Big_data = Big_data.reset_index(drop=True)
Big_data = Big_data.rename(columns={"latitude": "latitude", "longitude ": "longitude"})
Big_data

"""Cleaning data3"""

#data3
#read with excel
#ข้อมูลและลักษณะการเก็บต่างจาก Big data
data3 = pd.read_excel(local+'UFOs_coord.xlsx')
#Date/Time เก็บคนและ format และบางตัวไม่ได้เก็บเวลาไว้
data3['Time System'] = np.where(data3['Date / Time'].str.len() == 14, True, False)
data3['Time System2'] = np.where(data3['Date / Time'].str.len() == 13, True, False)
data3['Date / Time'][data3['Date / Time'].str.len() == 8] = np.nan
data3['Date / Time'][data3['Date / Time'].str.len() == 7] = np.nan
#data3['Time System'][data3['Time System'] == True] 
data3[data3['Time System'] == True] 
data3['Date / Time'][data3['Time System'] == True]
data3['Date / Time'][data3['Time System'] == True] = pd.to_datetime(pd.to_datetime(data3['Date / Time'][data3['Time System'] == True], format='%m/%d/%y %H:%M'),format='%d%b%Y %H:%M')
data3['Date / Time'][data3['Time System2'] == True] = pd.to_datetime(pd.to_datetime(data3['Date / Time'][data3['Time System'] == True], format='%m/%d/%y %H:%M'),format='%d%b%Y %H:%M')

data3['Country'][data3['Country'] == 'USA'] = 'us'
data3['Country'][data3['Country'] == 'CANADA'] = 'ca'
data3['State'] = data3['State'].str.lower()
data3['Shape'] = data3['Shape'].str.lower()
data3['City'] = data3['City'].str.lower()

data3.pop('Time System')
data3.pop('Time System2')
data3.columns = ['datetime', 'country','city','state','shape','comments','latitude','longitude']
data3.insert(5,'duration (seconds)',np.nan*5177,True)
data3.insert(6,'duration (hours/min)',np.nan*5177,True)
data3.insert(7,'date posted',np.nan*5177,True)
data3['datetime'] = pd.to_datetime(pd.to_datetime(data3['datetime'], format='%Y-%m-%d %H:%M:%S'), format='%d%b%Y %H:%M')
data3.info()
data3.head()

Big_data = pd.concat([Big_data,data3])
Big_data = Big_data.reset_index(drop=True)
Big_data.info()
Big_data.head()

#Outliner
from scipy import stats
import seaborn as sns
sns.set_theme(style="ticks")
#BEFORE REMOVE OUTLIER
# Initialize the figure with a logarithmic x axis
f, ax = plt.subplots(figsize=(7, 6))
ax.set_xscale("log")

# Load the example planets dataset
#planets = sns.load_dataset("sns_box")

# Plot the orbital period with horizontal boxes
sns.boxplot(x="duration (seconds)", y="shape", data=Big_data,
            whis=[0, 100], width=.6, palette="vlag")

# Add in points to show each observation
sns.stripplot(x="duration (seconds)", y="shape", data=Big_data,
              size=4, color=".3", linewidth=0)

# Tweak the visual presentation
ax.xaxis.grid(True)
ax.set(ylabel="")
sns.despine(trim=True, left=True)

#Outlier process
Q1 = Big_data['duration (seconds)'].quantile(0.25)
Q3 = Big_data['duration (seconds)'].quantile(0.75)
IQR = Q3 - Q1
Big_data['Outlier'] = np.where((Big_data['duration (seconds)'] < (Q1 - 1.5 * IQR)) | (Big_data['duration (seconds)'] > (Q3 + 1.5 * IQR)), True, False)
Big_data_afclean = Big_data[Big_data['Outlier'] == False]
Big_data_afclean.head()
Big_data_afclean.pop('Outlier')
Big_data_afclean.reset_index(drop=True)
Big_data = Big_data_afclean
sns.set_theme(style="ticks")
#After cleaning outlier
# Initialize the figure with a logarithmic x axis
f, ax = plt.subplots(figsize=(7, 6))
ax.set_xscale("log")

# Load the example planets dataset
#planets = sns.load_dataset("sns_box")

# Plot the orbital period with horizontal boxes
sns.boxplot(x="duration (seconds)", y="shape", data=Big_data,
            whis=[0, 100], width=.6, palette="vlag")

# Add in points to show each observation
sns.stripplot(x="duration (seconds)", y="shape", data=Big_data,
              size=4, color=".3", linewidth=0)

# Tweak the visual presentation
ax.xaxis.grid(True)
ax.set(ylabel="")
sns.despine(trim=True, left=True)
Big_data = Big_data.reset_index()

"""**FINISHED CLEANNING** : 
ให้นำข้อมูลชื่อ Big data ไปใช้

# EDA

เวลาของ UFO
"""

#รูปร่างของ UFO ที่พบนานที่สุด
data_duration = Big_data.groupby('shape').mean()
data_duration = data_duration.sort_values(by='duration (seconds)')
#Plot
fig_dims = (6, 10)
fig, ax = plt.subplots(figsize=fig_dims)
sns.set_theme(style="whitegrid")
ax = sns.barplot(x="duration (seconds)", y=data_duration.index, data=data_duration, ax=ax)
data_duration['duration (seconds)'].idxmax()

"""UFO รูปร่างที่มีผู้พบเห็นเป็นเวลานานที่สุดคือ round

จำนวนรูปร่างของ UFO
"""

#plot ปริมาณ UFO แต่ละประเภท
import seaborn as sns
#สร้าง data set ใหม่
UFO_shape_freq = Big_data.pivot_table(index=['shape'], aggfunc='size') #pivot_table ใช้ count
print(UFO_shape_freq.idxmax())

#Using matplotlib
pie, ax = plt.subplots(figsize=[10,20])
labels = UFO_shape_freq.keys()
plt.pie(x=UFO_shape_freq, explode=[0.05]*28, labels=labels, pctdistance=0.5)
plt.title("UFO frequency shape", fontsize=14);
pie.savefig("UFOPieChart.png")

"""UFO รูปร่างที่มีผู้พบเห็นมากที่่สุดคือ Light"""

country = Big_data.pivot_table(index=['country'], aggfunc='size')
country = pd.DataFrame(country).reset_index().sort_values(by=0)
sns.set_theme(style="whitegrid")
sns.color_palette("pastel")
ax = sns.barplot(x="country", y=0, data=country)
country.head()

"""จำนวนของ UFO ส่วนใหญ่เกิดที่ประเทศอเมริกา"""

Big_data['latitude'][Big_data['latitude'].isna() == True]
Big_data_p = Big_data.drop(38135)

def cal_percentage(data):
  info = data
  result = []
  sum = info.sum()
  result = [(j*100)/sum for j in info]
  result = np.array(result)
  return result
def toUpper(string):
  return string.upper()

US = Big_data_p[Big_data_p['country'] == 'us'].pivot_table(index=['state','country'], aggfunc='size')
US = pd.DataFrame(data=US)
US = US.assign(percent = cal_percentage(US.values))
US = US.reset_index()
US['state'] = US['state'].apply(toUpper)
US

"""DC"""

AU = Big_data_p[Big_data_p['country'] == 'au'].pivot_table(index=['state','country'], aggfunc='size')
AU = pd.DataFrame(data=AU)
AU = AU.assign(percent = cal_percentage(AU.values))

GB = Big_data_p[Big_data_p['country'] == 'gb'].pivot_table(index=['state','country'], aggfunc='size')
GB = pd.DataFrame(data=GB)
GB = GB.assign(percent = cal_percentage(GB.values))

CA = Big_data_p[Big_data_p['country'] == 'ca'].pivot_table(index=['state','country'], aggfunc='size')
CA = pd.DataFrame(data=CA)
CA = CA.assign(percent = cal_percentage(CA.values))

import os
import json
import requests
import folium
url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'
state_geo = f'{url}/us-states.json'
state_unemployment = f'{url}/US_Unemployment_Oct2012.csv'
state_data = pd.read_csv(state_unemployment)

m = folium.Map(location=[48, -102], zoom_start=3)

folium.Choropleth(
    geo_data=state_geo,
    name='choropleth',
    data=US,
    columns=['state', 'percent'],
    key_on='feature.id',
    fill_color='YlGn',
    fill_opacity=1,
    line_opacity=0.2,
    legend_name='Percent of UFO (%)'
).add_to(m)

m

"""# Prediction Fail"""

#Concept ใส่ state duration แล้วแสดง longtitude กับ latitude
Big_data['duration (seconds)']
state = pd.concat([Big_data['datetime'],Big_data['state'],Big_data['country'],Big_data['duration (seconds)'],Big_data['shape'],Big_data['latitude'],Big_data['longitude']],axis=1)
US_pre = state[state['country'] == 'us']

#test prediction for multipleoutput
US_pre 
state = list(set(US_pre['state']))
state_list = dict()
for i in range(len(state)):
  state_list[state[i]] = i

'''
shape = list(set(US_pre['shape']))
shape_list = dict()
for i in range(len(shape)):
  shape_list[shape[i]] = i
shape_list.pop(np.nan)
'''

#state convert
state_convert = []
for i in US_pre['state']:
  for j in state_list.keys():
    if i == j :
      state_convert.append(state_list[j])
      break
print(state_convert)
'''
#shape convert
#ปัญหาคือ shape
count = 0
shape_convert = []
for i in US_pre['shape']:
  for j in shape_list.keys():
    if i == j :
      shape_convert.append(shape_list[j])
      break
print(shape_convert)
#US_pre['shape_convert'] = shape_convert
print(len(shape_convert))
'''

US_pre['state_convert'] = state_convert

#search state
inp = int(input("Enter your number : "))
for name, num in state_list.items():
    if num == inp:
        print(name)

from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split
# summarize prediction
print(np.concatenate((US_pre['latitude'],US_pre['longitude']),axis=0))
US_pre['duration (seconds)'][US_pre['duration (seconds)'].isnull()] = 0.0

com1 = np.array(US_pre['latitude']).reshape(len(US_pre['latitude']),1)
com2 = np.array(US_pre['longitude']).reshape(len(US_pre['longitude']),1)

X = US_pre[['duration (seconds)','state_convert']]
y = np.concatenate((com1,com2),axis=1)
X1, X2, y1, y2 = train_test_split(X, y, random_state=0,train_size=0.5)

#ได้ทราบปัญหาการ predict [[]]
#ลองใส่ feature เข้าไป 1 work!
#ลอง predict work!
#ยังไม่ได้ test คะแนน
#np.concatenate((np.array(US_pre['latitude']),np.array(US_pre['longitude'])),axis=1)
#predict สำเร็จ

rgf = MultiOutputRegressor(LinearRegression()).fit(X1, y1)
y_model = rgf.predict(X2)
print(rgf.score(y2,y_model))
y_model = rgf.predict(X1[X1['state_convert'] == 0])

y_plot = y_model[:5]
import folium
from branca.element import Figure
fig3=Figure(width=550,height=350)
m3=folium.Map(location=[34.155834,-119.202789],tiles='cartodbpositron')
fig3.add_child(m3)
for i in y_plot:
  folium.Marker(location=i).add_to(m3)
fig3

"""# Window timestamp"""

Big_data_wt = Big_data.sort_values(by='datetime')
del Big_data_wt['index']
Big_data_wt.reset_index()

"""# New prediction UFOs amount

Test Model and R2
"""

Big_pre = Big_data.copy()
del Big_pre['index']
del Big_pre['city']
del Big_pre['state']
del Big_pre['country']
del Big_pre['shape']
del Big_pre['duration (hours/min)']
del Big_pre['comments']
del Big_pre['date posted']
del Big_pre['latitude']
del Big_pre['longitude']
one = np.ones(shape=(143742))
#Big_pre.index = pd.to_datetime(Big_pre.index, unit='s')
Big_pre = Big_pre.set_index('datetime')
Big_pre['Total'] = one
Big_pre
daily = Big_pre.resample('D').sum()
daily
days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
for i in range(7):
    daily[days[i]] = (daily.index.dayofweek == i).astype(float)
daily['annual'] = (daily.index - daily.index[0]).days / 365.00
daily.head()

#test predict with train dataset
column_names = ['Total','Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun','annual']
X = daily[column_names]
y = daily['duration (seconds)']
X1, X2, y1, y2 = train_test_split(X, y, random_state=0,
                                  train_size=0.5)
model = LinearRegression(fit_intercept=False)
model.fit(X1, y1)
y2_predict = model.predict(X2)
r2_score(y2, y2_predict)

daily['predicted'] = model.predict(X)
daily[['duration (seconds)', 'predicted']].plot(alpha=0.5);

import random
set_date = []
set_total = []
start_date = datetime.date(2020, 1, 1, )
end_date = datetime.date(2020, 5, 17)
time_between_dates = end_date - start_date
days_between_dates = time_between_dates.days
for i in range(100):
  random_number_of_days = random.randrange(days_between_dates)
  random_date = start_date + datetime.timedelta(days=random_number_of_days)
  set_date.append(random_date) 
for i in range(100):
  set_total.append(random.randint(0, 20))
#set_date
#set_total
data_test = pd.DataFrame(set_date)
data_test = data_test.rename(columns = {0:'datetime'}) 
data_test = data_test.set_index('datetime')
data_test['Total'] = set_total
daily2 = data_test
daily2.index = pd.to_datetime(daily2.index)
days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
for i in range(7):
    daily2[days[i]] = (daily2.index.dayofweek == i).astype(float)
daily2['annual'] = (daily2.index - daily2.index[0]).days / 365.00
daily2
daily2['predicted'] = model.predict(daily2)
daily2['predicted'].plot(alpha=0.5)

"""ทราบการเติบโตของเวลาที่พบ UFO ในแต่ละวันโดยรวม records

# Bar plot Top 5 ของแต่ละประเทศ
"""

#find a top 5 of shape
def Find_Top5_shape(Data):
  result1 = []
  result2 = []
  tmp = ''
  for i in range(0,5):
    tmp = Data.idxmax()
    result1.append(tmp)
    result2.append(Data[tmp])
    Data = Data.drop(tmp)
  return result1,result2

Data_shape_plt = Big_data.pivot_table(index=['shape'], aggfunc='size')
#plot Top5 
plot_top5_1,plot_top5_2  = Find_Top5_shape(Data_shape_plt)
tttmp = pd.DataFrame({'shape' : plot_top5_1 ,
                      'frequency' : plot_top5_2},
                     index=plot_top5_1)
tttmp.plot.bar()

tmpDataUS = Big_data[Big_data['country'] == 'us']
Data_US_shape_plt = tmpDataUS.pivot_table(index=['shape'], aggfunc='size')
#plot Top5 of US
plot_top5_US1,plot_top5_US2  = Find_Top5_shape(Data_US_shape_plt)
tttmp = pd.DataFrame({'shape' : plot_top5_US1 ,
                      'frequency' : plot_top5_US2},
                     index=plot_top5_US1)
tttmp.plot.bar()

#ทำการplot รูปร่างที่มากที่สุดในประเทศ au
tmpDataAU = Big_data[Big_data['country'] == 'au']
Data_au_shape_plt = tmpDataAU.pivot_table(index=['shape'], aggfunc='size')
#plot Top5 of AU
plot_top5_AU1,plot_top5_AU2  = Find_Top5_shape(Data_au_shape_plt)
tttmp2 = pd.DataFrame({'shape' : plot_top5_AU1 ,
                      'frequency' : plot_top5_AU2},
                     index=plot_top5_AU1)
tttmp2.plot.bar()

#ทำการplot รูปร่างที่มากที่สุดในประเทศ ca
tmpDataCA = Big_data[Big_data['country'] == 'ca']
Data_CA_shape_plt = tmpDataCA.pivot_table(index=['shape'], aggfunc='size')
#plot Top5 of CA
plot_top5_CA1,plot_top5_CA2  = Find_Top5_shape(Data_CA_shape_plt)
tttmp3 = pd.DataFrame({'shape' : plot_top5_CA1 ,
                      'frequency' : plot_top5_CA2},
                     index=plot_top5_CA1)
tttmp3.plot.bar()

#ทำการplot รูปร่างที่มากที่สุดในประเทศ DE
tmpDataDE = Big_data[Big_data['country'] == 'de']
Data_DE_shape_plt = tmpDataDE.pivot_table(index=['shape'], aggfunc='size')
#plot Top5 of DE
plot_top5_DE1,plot_top5_DE2  = Find_Top5_shape(Data_DE_shape_plt)
tttmp3 = pd.DataFrame({'shape' : plot_top5_DE1 ,
                      'frequency' : plot_top5_DE2},
                     index=plot_top5_DE1)
tttmp3.plot.bar()

#ทำการplot รูปร่างที่มากที่สุดในประเทศ GB
tmpDataGB = Big_data[Big_data['country'] == 'gb']
Data_GB_shape_plt = tmpDataGB.pivot_table(index=['shape'], aggfunc='size')
#plot Top5 of GB
plot_top5_GB1,plot_top5_GB2  = Find_Top5_shape(Data_GB_shape_plt)
tttmp4 = pd.DataFrame({'shape' : plot_top5_GB1 ,
                      'frequency' : plot_top5_GB2},
                     index=plot_top5_GB1)
tttmp4.plot.bar()

"""# การพบเจอ UFO"""

tmpDataUS = tmpDataUS.sort_values(by='datetime')
tmpDataUS.datetime.max() #2016-12-21 19:15:00
tmpDataUS.datetime.min() #1910-01-02 00:00:00
#check ว่า ตั้งแต่ 1910 มี ปีไหนบ้างที่ซ้ำและปีไหนบ้างที่ไม่ปรากฎ
check_list = [ i*0 for i in range(107)]
tmpTime = []
for k in tmpDataUS['datetime']:
  tmpTime.append(k.year)
for i in tmpTime:
  for j in range(107) :
    if i == 1910+j :
      check_list[j] += 1

max(check_list)
MaxYear_InUs = 0
for i in range(len(check_list)):
  if check_list[i] == max(check_list):
    MaxYear_InUs += i
print('ใน US ปีที่่มีการพบเจอUFOเยอะที่สุด'+str(1910+MaxYear_InUs))

#ปีที่ไม่มีการพบเลย ใน US หรือ ไม่ระบุวันเวลาที่แน่ชัด
tmp_no_UFO_US = []
for i in range(len(check_list)):
  if check_list[i] == 0 :
    tmp_no_UFO_US.append(1910+i)
tmp_no_UFO_US

def check_time(Data):
  check_list = [ i*0 for i in range(107)]
  tmpTime = []
  for k in Data['datetime']:
    tmpTime.append(k.year)
  for i in tmpTime:
    for j in range(107) :
      if i == 1910+j :
        check_list[j] += 1
  return check_list

# ประเทศต่างๆ us au ca de gb
tmpDataCA = tmpDataCA.sort_values(by='datetime')
tmpDataAU = tmpDataAU.sort_values(by='datetime')
tmpDataDE = tmpDataDE.sort_values(by='datetime')
tmpDataGB = tmpDataGB.sort_values(by='datetime')
tmpDataCA.datetime.max() #2016-12-18 22:00:00
tmpDataCA.datetime.min() #1936-09-15 19:00:00
tmpDataAU.datetime.max() #2014-05-08 18:45:00
tmpDataAU.datetime.min() #1960-07-29 19:30:00
tmpDataDE.datetime.max() #2014-05-07 00:00:00
tmpDataDE.datetime.min() #1962-03-15 12:00:00
tmpDataGB.datetime.max() #2014-05-07 11:56:00
tmpDataGB.datetime.min() #1943-08-13 22:00:00
check_TimeCA = check_time(tmpDataCA)
check_TimeAU = check_time(tmpDataAU)
check_TimeDE = check_time(tmpDataDE)
check_TimeGB = check_time(tmpDataGB)
#หาปีที่ประเทศที่มีทั้งหมดไม่พบ UFO เลย
No_UFO = []
MaxYear_InCA = 0
MaxYear_InAU = 0
MaxYear_InDE = 0
MaxYear_InGB = 0
for i in range(107):
  if check_list[i] == 0 & check_TimeAU[i] == 0 & check_TimeCA[i] == 0 & check_TimeDE[i] == 0 & check_TimeGB[i] == 0 :
    No_UFO.append(1910+i)
  if check_TimeAU[i] == max(check_TimeAU) :
    MaxYear_InAU += i
    print('ใน AU ปีที่่มีการพบเจอUFOเยอะที่สุด'+str(1910+MaxYear_InAU))
  if check_TimeDE[i] == max(check_TimeDE) :
    MaxYear_InDE += i
    print('ใน DE ปีที่่มีการพบเจอUFOเยอะที่สุด'+str(1910+MaxYear_InDE))
  if check_TimeGB[i] == max(check_TimeGB) :
    MaxYear_InGB += i
    print('ใน GB ปีที่่มีการพบเจอUFOเยอะที่สุด'+str(1910+MaxYear_InGB))
  if check_TimeCA[i] == max(check_TimeCA) :
    MaxYear_InCA += i
    print('ใน CA ปีที่่มีการพบเจอUFOเยอะที่สุด'+str(1910+MaxYear_InCA))

#ปีที่ประเทศที่มีทั้งหมดไม่พบ UFO เลย หรือระบุไม่แน่ชัด
No_UFO

"""**ดังนั้นจะทำการพิจารณาการพบเห็นตั้งแต่ปี 1947 - 2016**"""

#ช่วงเวลาที่สังเกต คือ 69 ปี
#ดูว่า UFO ในแต่ละปี มีความสัมพันธิ์กันไหม
tmp_us1947_2016 = check_list[37::]
tmp_ca1947_2016 = check_TimeCA[37::]
tmp_au1947_2016 = check_TimeAU[37::]
tmp_de1947_2016 = check_TimeDE[37::]
tmp_gb1947_2016 = check_TimeGB[37::]
for i in range(0,69,3):
  print('ปีที่'+str(1947+i)+'ถึงปี'+str(1947+i+3)+'ของ US มีความถี่'+str(tmp_us1947_2016[i:i+3]))
  print('ปีที่'+str(1947+i)+'ถึงปี'+str(1947+i+3)+'ของ ca มีความถี่'+str(tmp_ca1947_2016[i:i+3]))
  print('ปีที่'+str(1947+i)+'ถึงปี'+str(1947+i+3)+'ของ au มีความถี่'+str(tmp_au1947_2016[i:i+3]))
  print('ปีที่'+str(1947+i)+'ถึงปี'+str(1947+i+3)+'ของ de มีความถี่'+str(tmp_de1947_2016[i:i+3]))
  print('ปีที่'+str(1947+i)+'ถึงปี'+str(1947+i+3)+'ของ gb มีความถี่'+str(tmp_gb1947_2016[i:i+3]))

"""**สิ่งที่ได้คือตั้งแต่ปี 1947-1995 มีความถี่การพบเจอการพบเจอของ UFO ของประเทศ ca au de gb ที่ต่ำมากไม่เกิน 30 ครั้ง แต่ ปรเทศ US กลับมีการพบเจอมากกว่าเสมอไม่ต่ำกว่า30ครั้งต่อปี เฉลี่ย 210 ครั้งต่อปี 
และตั้งแต่ปี 1995 ขึ้นมามีการพบเจอ ของทั้ง 5 ประเทศสูงขึ้นเรื่อยๆอย่างประหลาดจนกระทั้งปี 2013 ทั้ง 5 ประเทศก็มีจำนวนการพบน้อยลงอย่างมากเมื่อเทียบกลับปีก่อนๆจนกระทั้งปี 2016 การพบเจอ UFO ของแต่ละประเทศก็มีข้อมูลไม่แน่ชัดหรือไม่พบเจอเลย**
"""

sum = 0
for i in range(0,48):
  sum += tmp_us1947_2016[i]
total = sum/48
total